<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link rel="stylesheet" href="/assets/css/style.css?v=" media="screen" type="text/css">
    <link rel="stylesheet" href="/assets/css/print.css" media="print" type="text/css">

    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Model in your browser | Model inference in your browser using WASM and/or JS</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Model in your browser" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Model inference in your browser using WASM and/or JS" />
<meta property="og:description" content="Model inference in your browser using WASM and/or JS" />
<link rel="canonical" href="http://localhost:4000/" />
<meta property="og:url" content="http://localhost:4000/" />
<meta property="og:site_name" content="Model in your browser" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Model in your browser" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","description":"Model inference in your browser using WASM and/or JS","headline":"Model in your browser","name":"Model in your browser","url":"http://localhost:4000/"}</script>
<!-- End Jekyll SEO tag -->


    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->

  </head>

  <body>
    <header>
      <div class="inner">
        <a href="http://localhost:4000/">
          <h1>Model in your browser</h1>
        </a>
        <h2>Model inference in your browser using WASM and/or JS</h2>
        
        
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">
          <h2 id="why-do-we-try-to-do-it">Why do we try to do it?</h2>

<p>Imagine that you have a decent model that could make using your application/website more interesting, but you:</p>
<ul>
  <li>don‚Äôt want to pay for hosting the model for inference and associated infrastructure</li>
  <li>make use of the end user hardware</li>
  <li>make use (if possible) of the end user GPU</li>
  <li>make it distributable to every architecture</li>
  <li>make the inference efficient (by using WASM)</li>
</ul>

<p>WASM itself already partly answers the problem.</p>

<h2 id="wasm">WASM</h2>

<p>WebAssembly (abbreviated¬†<em>Wasm</em>) is a binary instruction format for a stack-based virtual machine. Wasm is designed as a portable compilation target for programming languages, enabling deployment on the web for client and server applications.</p>

<blockquote>
  <p><em>WebAssembly (Wasm) is generally faster than JavaScript for certain types of tasks. WebAssembly is a binary instruction format that allows developers to run code at near-native speed in modern web browsers. JavaScript, on the other hand, is a high-level scripting language that is often used for web development.</em></p>
</blockquote>

<p>https://graffersid.com/webassembly-vs-javascript/</p>

<p>So, it would be nice to have the inference part of your code written in WASM, to make it more optimized.</p>

<h2 id="how-to-run-models-with-wasm-in-the-browser">How to run models with WASM in the browser</h2>

<p>First, I would like to clarify that using WASM in the following examples practically means installing packages which contain WASM binaries or have been compiled entirely to WASM. 
Operations and functions which would run slower using vanilla JS can be rewritten in RUST and compiled to WASM, but JavaScript still interacts with WASM.</p>

<ul>
  <li>
    <h3 id="transformersjs">Transformers.js</h3>
  </li>
</ul>

<p>Transformers.js is designed to be functionally equivalent to Hugging Face‚Äôs transformers python library, meaning you can run the same pretrained models using a very similar API. The models support common tasks in different modalities, such as:</p>
<ul>
  <li>üìù Natural Language Processing: text classification, named entity recognition, question answering, language modeling, summarization, translation, multiple choice, and text generation.</li>
  <li>üñºÔ∏è Computer Vision: image classification, object detection, and segmentation.</li>
  <li>üó£Ô∏è Audio: automatic speech recognition and audio classification.</li>
  <li>üêô Multimodal: zero-shot image classification.</li>
</ul>

<p>Transformers.js uses ONNX Runtime to run models in the browser. 
If you don‚Äôt manage to find one of the numerous models from Hugging Face tailored to your needs (or you have finetuned your own model and want to use with Transformers.js), you can follow the path of converting your model, from popular formats (PyTorch, Tensorflow) to ONNX. 
(Transformers.js](https://huggingface.co/docs/transformers.js)
The <a href="https://github.com/huggingface/optimum#onnx--onnx-runtime">conversion to ONNX format</a> (in case of Transformers.js for ONNX runtime) is rather easy with these commands:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install --upgrade-strategy eager optimum[onnxruntime]
optimum-cli inc quantize --model distilbert-base-cased-distilled-squad --output ./quantized_distilbert
</code></pre></div></div>
<p>As you can see optimum-cli makes it easy to perform quantization of your model, which allows to make a smaller file from the model weights. 
It is a good practice to load your model to verify that its accuracy after quantization is still acceptable for your use.</p>

<p>Advantages:</p>
<ul>
  <li>Transformers.js library is very well documented</li>
  <li>extremely easy to use (syntax similar to HF transformers)</li>
  <li>surprisingly fast</li>
</ul>

<p>Disadvantages:</p>

<p>As of now WebGPU support is not included. This means, that running larger models will be constrained to CPU architecture and may not be efficient.
However I recommend checking out their <a href="https://xenova.github.io/transformers.js/">projects repo</a> for some impressive examples, they may be fast enough and not need GPU support.</p>

<h3 id="simple-transformersjs-app">Simple Transformers.js app</h3>
<p>I experimented with one of the examples on Transformers.js Github page: <a href="https://github.com/xenova/transformers.js/tree/main/examples/react-translator">react-translator</a> to convert it to a <a href="https://github.com/justyna3773/text_manipulation_transformers_js.git">simple question answering app</a>.
I was impressed with how adaptable this example is, the only thing you have to change the pipeline from <em>translation</em> to <em>text2text-generation</em> to make the model generate answer instead of translating is MyTranslationPipeline class in worker.js file:</p>

<div class="language-js highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="p">{</span> <span class="nx">pipeline</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">'</span><span class="s1">@xenova/transformers</span><span class="dl">'</span><span class="p">;</span>

<span class="kd">class</span> <span class="nc">MyTranslationPipeline</span> <span class="p">{</span>
    <span class="kd">static</span> <span class="nx">task</span> <span class="o">=</span> <span class="dl">'</span><span class="s1">text2text-generation</span><span class="dl">'</span><span class="p">;</span>
    <span class="kd">static</span> <span class="nx">model</span> <span class="o">=</span> <span class="dl">'</span><span class="s1">Xenova/flan-t5-base</span><span class="dl">'</span><span class="p">;</span>
    <span class="kd">static</span> <span class="nx">instance</span> <span class="o">=</span> <span class="kc">null</span><span class="p">;</span>

    <span class="kd">static</span> <span class="k">async</span> <span class="nf">getInstance</span><span class="p">(</span><span class="nx">progress_callback</span> <span class="o">=</span> <span class="kc">null</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">if </span><span class="p">(</span><span class="k">this</span><span class="p">.</span><span class="nx">instance</span> <span class="o">===</span> <span class="kc">null</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">this</span><span class="p">.</span><span class="nx">instance</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span><span class="k">this</span><span class="p">.</span><span class="nx">task</span><span class="p">,</span> <span class="k">this</span><span class="p">.</span><span class="nx">model</span><span class="p">,</span> <span class="p">{</span> <span class="nx">progress_callback</span> <span class="p">});</span>
        <span class="p">}</span>

        <span class="k">return</span> <span class="k">this</span><span class="p">.</span><span class="nx">instance</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span>

</code></pre></div></div>
<p>Essentially you change the name of the task and model ID from HuggingFace.
After that you run <code class="language-plaintext highlighter-rouge">npm run dev</code> and voila, you can access your App under local address: <a href="">http://localhost:5173/</a></p>

<hr />

<h2 id="webgpu">WebGPU</h2>

<p>WebGPU exposes an API for performing operations, such as rendering and computation, on a Graphics Processing Unit.
It is still in the early stages, but Chrome enables it by default since April 2023 (version 113).</p>

<p><img src="web_gpu_stack.png" alt="webgpu" /></p>

<p><img src="implementation_status.png" alt="implementation" /></p>

<h2 id="project-which-use-webgpu-tested-with-nvidia-rtx-3060-on-a-laptop">Project which use WebGPU (tested with NVIDIA RTX 3060 on a laptop)</h2>
<ul>
  <li>
    <h3 id="web-llm">Web-LLM</h3>
    <blockquote>
      <p><em>WebLLM is a modular, customizable javascript package that directly brings language model chats directly onto web browsers with hardware acceleration.¬†<strong>Everything runs inside the browser with no server support and is accelerated with WebGPU.</strong>¬†We can bring a lot of fun opportunities to build AI assistants for everyone and enable privacy while enjoying GPU acceleration.</em>
<a href="https://webllm.mlc.ai/">Web-LLM demo</a></p>
    </blockquote>
  </li>
</ul>

<p>Advantages:
If you are fine with your user downloading large model files then it works well. It also has a pretty extensive documentation.</p>

<p>Problems:</p>
<ul>
  <li>initial downloading of the model is slow and the models supported are heavy (several GB). TinyLLama for example, takes 2 GB of your memory.</li>
  <li>I experimented with disabling GPU support in the browser - then the chat does not work at all. This undermines the idea that we can have one app to run on all hardwares.</li>
  <li>customization capabilities - there is a limited number of model architectures it supports (Mistral, LLama, Red Pyjamas and a few others). You can add model weights from your finetuned model in easy fashion, however adding your own architecture is problematic.</li>
</ul>

<p>Anyway, this project seems to have a lot of potential and well documented workflow on how to add your own model library. 
I wanted to add a small BERT model, to make it decently good for conversations or question but at the same time keep it as small as possible.<br />
I followed their workflow for adding new model library: <a href="https://llm.mlc.ai/docs/deploy/javascript.html#bring-your-own-model-library">https://llm.mlc.ai/docs/deploy/javascript.html#bring-your-own-model-library</a> and couldn‚Äôt reproduce the notebook results: <a href="https://github.com/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_add_new_model_architecture_in_tvm_nn_module.ipynb">tutorial_add_new_model_architecture_in_tvm_nn_module.ipynb</a>
Here is the link to the issue: <a href="https://github.com/mlc-ai/web-llm/issues/274">https://github.com/mlc-ai/web-llm/issues/274</a></p>

<ul>
  <li>
    <h3 id="web-stablediffusion">Web-StableDiffusion</h3>
    <p>https://github.com/mlc-ai/web-stable-diffusion</p>
    <blockquote>
      <p><em>This project brings stable diffusion models onto web browsers.¬†<strong>Everything runs inside the browser with no server support.</strong></em>¬†</p>
    </blockquote>
  </li>
  <li>Similar problems to Web-LLM - you have to have a lot of memory, as it downloads models which are not lightweight (but at least they are cached and you get a progress update while they download)</li>
  <li>
    <p>It is slow in the first run</p>
  </li>
  <li>
    <h3 id="web-onnx">Web ONNX</h3>
  </li>
</ul>

<blockquote>
  <p>Wonnx is a GPU-accelerated ONNX inference run-time written 100% in Rust, ready for the web.
(https://github.com/webonnx/wonnx)</p>
</blockquote>

<p>What is ONNX runtime? Open Neural Network Exchange Runtime is a cross-platform inference and training machine-learning accelerator.
ONNX Runtime inference can enable faster customer experiences and lower costs, supports models from PyTorch and TensorFlow/Keras as well as classical machine learning libraries such as scikit-learn, LightGBM, XGBoost, etc. ONNX Runtime is compatible with different hardware, drivers, and operating systems, and provides optimal performance by leveraging hardware accelerators where applicable alongside graph optimizations and transforms.</p>

<p>Web ONNX has a WASM package version and can be installed with <code class="language-plaintext highlighter-rouge">npm install @webonnx/wonnx-wasm</code>
I used their complete <a href="https://github.com/webonnx/wonnx-wasm-example">example with squeeze model</a> for image classification with Vite bundler (bundler optimizes the project structure and enables communication between components written in different languages)
As part of this exercise I sought to bring this simple example to Github pages. 
It requires installing gh-pages package <code class="language-plaintext highlighter-rouge">npm install gh-pages</code> and updating the package.json file with the following lines:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"scripts":
"predeploy": "npm run build",
"deploy": "gh-pages -d dist",
</code></pre></div></div>
<p>As well as adding your Github pages link in the package.json file with:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  "homepage": "https://justyna3773.github.io/camera_classify_webgpu/",
</code></pre></div></div>
<p>After that you can deploy the app to Github pages with <code class="language-plaintext highlighter-rouge">npm run deploy</code>
I did not manage, however, to overcome the problem with no inference being made when GPU is not available.
Link to the repo for real-time object classification from the camera: <a href="https://github.com/justyna3773/camera_classify_webgpu">https://github.com/justyna3773/camera_classify_webgpu</a> and to the deployed app: <a href="https://justyna3773.github.io/camera_classify_webgpu/">https://justyna3773.github.io/camera_classify_webgpu/</a></p>

<p>When I tried to test WONNX with a small language model like BERT (since the authors claim that WONNX is tested with BERT architecture), I encountered a problem with inferring shapes when trying to prepare the model for inference with WONNX <a href="https://github.com/webonnx/wonnx/issues/200">https://github.com/webonnx/wonnx/issues/200</a>. As of now, it hasn‚Äôt been resolved. 
In general, running language models with WONNX may be problematic, as the package does not implement all necessary operators for BERT architectures to work. For more details take a look at this post by one of major WONNX contributors: <a href="https://t-shaped.nl/2023/running-ai-models-in-the-browser-using-wonnx">https://t-shaped.nl/2023/running-ai-models-in-the-browser-using-wonnx</a></p>

<p>Summary: WONNX is helpful when you try to distribute models for image classification, like Squeezenet or ResNet, however it is not easily customized for usage with language models.</p>

        </section>

        <aside id="sidebar">

          

          <p>This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</p>
        </aside>
      </div>
    </div>

  </body>
</html>
